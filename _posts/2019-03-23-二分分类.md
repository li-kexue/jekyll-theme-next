---
layout: post
title: "二分分类"
date: 2019-03-23
categories: 机器学习
photos:
- https://images.unsplash.com/photo-1553283170-53414ec5177f?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=500&q=60
tags: [机器学习]
---
## 二分分类
### 公式
当$x$只有一个属性时：
$f(x)=wx+b$
当$x$的属性不止一个时：
$$x=\begin{bmatrix} x1\\ x2\\x3\end{bmatrix}  
w=\begin{bmatrix}w1\\w2\\w3\end{bmatrix}  
f(x)=w1x1+w2x2+w3x3+b$$
这时我们就可以转换成矩阵运算了
$f(x)=w^Tx$
即  
$$\begin{bmatrix}w1&w2&w3\end{bmatrix} \cdot \begin{bmatrix}x1\\x2\\x3\end{bmatrix} +b=w1x1+w2x2+w3x3+b$$


#### logistic 回归
对于二分分类问题，我们最终想要的结果是0和1，但实际情况很难得到这样的结果。
所以我们就引入了激活函数，在二分分类问题中我们一般使用$\sigma$函数，这个函数公式为：$f(x)= \frac{1}{1+e^{-x}}$。这个函数的图像为
![avater](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1553323790234&di=eb4d65178e50fc0339e4e60cc198a21e&imgtype=0&src=http://www.elecfans.com/uploads/allimg/180110/1231556091-3.png)
其结果都在0~1之间,我们可以通过四舍五入，将所有结果置为整数。
我们的公式就变为：$\hat{y}=\sigma(w^Tx+b)$,这里的$\hat{y}$就是我们要求的预测值。
接着就引出了我们的代价函数（成本函数），他的公式为：
单个样本：
$$L(\hat{y},y)=-(y\log{\hat{y}}+(1-y)\log({1-\hat{y})})$$  
m个样本:  
$$J(w,b)=\frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}\log\hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)}))$$
#### 推导
开始之前希望大家了解一些极大似然估计：https://blog.csdn.net/u014182497/article/details/82252456
之前我们求的$\hat{y}$的范围处于0~1,这样我们就可以将他的值看做$p(y=1|x)$的概率了，于是$p(y=1|x)=\hat{y},则p(y=0|x)=1-\hat{y}$。  
$$f(y;p)= \left\{\begin{matrix}p&y=1 \\1-p&y=0 \end{matrix}\right.$$  
我们可以将这个式子整合到一起，变成$f(y;p)=p^y(1-p)^{(1-y)}$,因为$y\in(0,1)$,
当y=1时，$f(y;p)=p^1(1-p)^{(1-1)}=p$,
当y=0时，$f(y;p)=p^0(1-p)^{1-0}=1-p$，满足上面的式子。
$L=p^y(1-p)^{(1-y)}=\hat{y}^y(1-\hat{y})^{(1-y)}$然后两边同时取log，得到$logL=ylog\hat{y}+(1-y)log(1-\hat{y})$,最后我们的成本函数又取$L(\hat{y},y)=-logL$,取这个的原因：https://www.jianshu.com/p/1bf35d61995f
#### 梯度下降
注：接下来将用$da$表示$\frac{dL(a,y)}{da}$,$dz$表示$\frac{dL(a,y)}{dz}$，$dw$表示$\frac{dL(a,y)}{dw}$
首先列出下列公式：
$z=w^Tx+b$
$\hat{y}=a=\sigma(z)=\frac{1}{1+e^{-(w^T+b)}}$
$L(a,y)=-(yloga+(1-y)log(1-a))$
1.计算$da$
$da=\frac{dL(a,y)}{da}=-\frac{y}{a}+\frac{1-y}{1-a}$
2.计算$dz$
这时我们需要用到链式法则
$dz=\frac{dL(a,y)}{dz}=\frac{dL(a,y)}{da}\frac{da}{dz}=(-\frac{y}{a}+\frac{1-y}{1-a})\frac{da}{dz}$
此时需要求$\frac{da}{dz}$，我们可以设$1+e^{(w^T+b)}=x$，则变成对$\frac{1}{x}$求导，对$\frac{1}{x}$求导等于
$-(\frac{1}{x^2}x')=-(\frac{1}{(1+e^{-z})^2})(-e^{-z})$

$=\frac{1+e^{-z}-1}{(1+e^{-z})^2}$

$=\frac{1}{1+e^{-z}}-(\frac{1}{1+e^{-z}})^2$

$=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})$

$=a(1-a)$
所以$dz=a(1-a)(-\frac{y}{a}+\frac{1-y}{1-a})=a-y$
3.计算$dw$
同样利用链式法则
$dw=\frac{dL(a,y)}{dw}=(\frac{dL(a,y)}{da}\frac{da}{dz})\frac{dz}{dw}$
$\frac{dz}{dw}=x$
所以$dw=xdz$,这里的$dz=\frac{dL(a,y)}{dz}$
4.计算db
$db=\frac{dL(a,y)}{dw}=(\frac{dL(a,y)}{da}\frac{da}{dz})\frac{dz}{db}$
$\frac{dz}{db}=1$
所以$db=dz$,这里的$dz=\frac{dL(a,y)}{dz}$
接着就可以进行迭代，这里的$\alpha$是学习率
$w:=w-\alpha dw$
$b:=b-\alpha db$

刚入门，还有很多不足之处，请大家多多指出。
