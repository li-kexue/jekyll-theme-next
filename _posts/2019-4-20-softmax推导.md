---
layout: post
title: "softmax推导"
date: 2019-04-20
categories: 机器学习
photos:
- https://images.unsplash.com/photo-1553283170-53414ec5177f?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=500&q=60
tags: [机器学习]
---

### softmax公式
$a_j=\frac{e^{Z_j}}{\sum_{i=1}^{m}e^{Z_i}}$

简单解释一下这个公式，$a_j$表示最终输出第j类的概率，而第j类的概率是用$e^{Z_j}$比上所有类别的和。
### 损失函数
$L=-\sum_i{y_i\ln{a_i}}$
损失函数用这个而不用平方差的形式，简单的说是因为这个训练出的模型效果更好。
详细解释请看这篇博客：[平方损失函数与交叉熵损失函数][1]

### 求导
先约定一下：$dZ_i$表示$\frac{\partial L}{\partial Z_i}$

$dZ_i=\frac{\partial L}{\partial Z_i}=\sum_j{(\frac{\partial L_j}{\partial a_j}\frac{\partial a_j}{\partial Z_i})}=\sum_j{(\frac{\partial {(-y_j\ln{a_j}})}{\partial a_j}\frac{\partial a_j}{\partial Z_i})}$

先不看求和符号，分成两部分$\frac{\partial {(-y_j\ln{a_j}})}{\partial a_j},\frac{\partial a_j}{\partial Z_i}$进行求导。

$$\frac{\partial {(-y_j\ln{a_j}})}{\partial a_j}=-\frac{y_j}{a_j}$$

$\frac{\partial a_j}{\partial Z_i}$,这部分比较特殊，i和j代表不同的数，i表示一个特定的数，而j表示任意一个节点。
所以我们分两种情况：
**1、i=j时：**$$\frac{\partial a_j}{\partial Z_i}=\frac{\partial a_i}{\partial Z_i}=\frac{\partial 
{\frac{e^{Z_i}} {\sum_k e^{Z_k}}}}{\partial Z_i}=\frac{e^{Z_i}\sum_k{e^{Z_k}}-(e^{Z_i})^2}{(\sum_k{e^{Z_k}})^2}$$

解释一下$(e^{Z_i})^2$是怎么来的，对$\sum_k{e^{Z_k}}$求导，等于对$e^{Z_1}+e^{Z_2}+...e^{Z_i}+...+e^{Z_n}$求导，因为k是从一开始的，肯定有一个k值和i相等，而其他$e^{Z_k}$相对于$Z_i$都是常数，求导变成零，只剩下$e^{Z_i}$，所以求导为上面的形式。继续推导。
上式=$$\frac{e^{Z_i}}{\sum_ke^{Z_k}}(1-\frac{e^{Z_i}}{\sum_ke^{Z_k}})=a_i(1-a_i)$$

**2、$i\neq j$时：**
$$\frac{\partial a_j}{\partial Z_i}=\frac{\partial 
{\frac{e^{Z_j}} {\sum_{k}e^{Z_k}}}}{\partial Z_i}=\frac{-e^{Z_i}e^{Z_j}}{(\sum_{k}{e^{Z_k}})^2}=-a_ia_j$$
这里对$\sum_k{e^{Z_k}}$求导方法和上面一样。
最后将上面的导数合并一下
$$\sum_j{(\frac{\partial L_j}{\partial a_j}\frac{\partial a_j}{\partial Z_i})}=\sum_{j\neq i}{(\frac{\partial L_j}{\partial a_j}\frac{\partial a_j}{\partial Z_i})}+\sum_{j=i}{(\frac{\partial L_j}{\partial a_j}\frac{\partial a_j}{\partial Z_i})}$$
$$=\sum_{j\neq i}{-\frac{y_j}{a_j}(-a_ia_j)}+\sum_{j=i}{-\frac{y_i}{a_i}}a_i(1-a_i)$$
$$=(\sum_{j\neq i}{a_iy_j})+(a_i-1)y_i=((a_i\sum_{j\neq i}y_j)+a_iy_i)-y_i$$
$$=a_i(\sum_jy_j)-y_i=a_i-y_i$$

最后我想说明一下，$((a_i\sum_{j\neq i}y_j)+a_iy_i)$这里我用了两个括号，就是想给大家一个提示，因为$a_i\sum_{j\neq i}y_j$这里少了一个$j=i$的点，而后面有一个$a_iy_i$,合起来正好是$a_i(\sum_jy_j)$；还有一点是最后的结果为什么去掉求和符号，对于分类问题$\sum_jy_j$等于1。
举个例子：
$\begin{bmatrix}
y1\\ y2
\\ y3
\end{bmatrix}$表示最终要分的三个类别，而分的类别最终只能有一种结果，所以$y1,y2,y3$中只有一个为1，其他全为零，所以求和为1。

softmax的案例将会在下一篇博客中详细介绍。





[1]:https://blog.csdn.net/m_buddy/article/details/80224409
